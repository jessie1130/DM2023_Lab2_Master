{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name:唐孟婕\n",
    "\n",
    "Student ID:112062539\n",
    "\n",
    "GitHub ID:jessie1130\n",
    "\n",
    "Kaggle name:Jessie\n",
    "\n",
    "Kaggle private scoreboard snapshot:\n",
    "\n",
    "[Snapshot](img/pic0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home** exercises in the DM2023-Lab2-master. You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/t/09b1d0f3f8584d06848252277cb535f2) regarding Emotion Recognition on Twitter by this link https://www.kaggle.com/t/09b1d0f3f8584d06848252277cb535f2. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the 30% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (60-x)/6 + 20 points, where x is your ranking in the leaderboard (ie. If you rank 3rd your score will be (60-3)/6 + 20 = 29.5% out of 30%)   \n",
    "    Submit your last submission __BEFORE the deadline (Dec. 27th 11:59 pm, Wednesday)_. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developping the model for the competition (You can use code and comment it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook** and **add minimal comments where needed**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the link to it on the corresponding e-learn assignment.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Dec. 31th 11:59 pm, Sunday)__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 1 (Take home): **  \n",
    "Plot word frequency for Top 30 words in both train and test dataset. (Hint: refer to DM lab 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'to', 'and', 'is', 'of', 'it', 'you', 'in', 'that', 'my', 'for', 'on', 'be', 'me', 'this', 'so', 'have', 'just', 'but', 'not', 'with', 'can', 'at', 'like', 'all', 'when', 'are', 'if', 'your', 'was']\n",
      "['the', 'to', 'and', 'is', 'of', 'it', 'in', 'you', 'that', 'for', 'my', 'on', 'be', 'me', 'so', 'can', 'just', 'at', 'have', 'all', 'was', 'are', 'but', 'with', 'your', 'not', 'up', 'this', 'get', 'like']\n"
     ]
    }
   ],
   "source": [
    "# Answer here\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect_train = CountVectorizer()\n",
    "count_vect_test = CountVectorizer()\n",
    "train_counts = count_vect_train.fit_transform(train_df.text) #learn the vocabulary and return document-term matrix\n",
    "test_counts = count_vect_test.fit_transform(test_df.text)\n",
    "\n",
    "#use sparse matrix to find the count of every term in every documents\n",
    "from collections import Counter \n",
    "from scipy.sparse import csr_matrix, find\n",
    "\n",
    "train_matrix = csr_matrix(train_counts.toarray())\n",
    "train = find(train_matrix)\n",
    "train_term = list(train[1])\n",
    "test_matrix = csr_matrix(test_counts.toarray())\n",
    "test = find(test_matrix)\n",
    "test_term = list(test[1])\n",
    "\n",
    "#use counter to find the 30 terms appear\n",
    "train_counters = Counter(train_term)\n",
    "train_most = train_counters.most_common(30)\n",
    "test_counters = Counter(test_term)\n",
    "test_most = test_counters.most_common(30)\n",
    "\n",
    "# the 30 most terms\n",
    "plot_train = []\n",
    "for i in train_most:\n",
    "    plot_train.append(count_vect_train.get_feature_names_out()[i[0]])\n",
    "print(plot_train)\n",
    "plot_test = []\n",
    "for i in test_most:\n",
    "    plot_test.append(count_vect_test.get_feature_names_out()[i[0]])\n",
    "print(plot_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 2 (Take home): **  \n",
    "Generate an embedding using the TF-IDF vectorizer instead of th BOW one with 1000 features and show the feature names for features [100:110]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jessie/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['awful', 'b', 'baby', 'back', 'bad', 'ball', 'balls', 'bb18', 'bc',\n",
       "       'bday'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Answer here\n",
    "#TF-IDF\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# build analyzers (TF-IDF)\n",
    "TF_vectorizer = TfidfVectorizer(max_features=1000, tokenizer=nltk.word_tokenize)\n",
    "\n",
    "# apply analyzer to training data\n",
    "TF_vectorizer.fit(train_df['text'])\n",
    "\n",
    "TF_features = TF_vectorizer.transform(train_df['text'])\n",
    "\n",
    "## check dimension\n",
    "TF_features.shape\n",
    "\n",
    "feature_names_500_TF = TF_vectorizer.get_feature_names_out()\n",
    "feature_names_500_TF[100:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 3 (Take home): **  \n",
    "Can you interpret the results above? What do they mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'joy' is very differeny from 3 other emotions, so it has higher accuracy. 'anger', 'fear' and 'sadness' are all negative emotions, some cases would be classified wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 4 (Take home): **  \n",
    "Build a model using a ```Naive Bayes``` model and train it. What are the testing results? \n",
    "\n",
    "*Reference*: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (3613, 1000)\n",
      "y_train.shape:  (3613,)\n",
      "X_test.shape:  (347, 1000)\n",
      "y_test.shape:  (347,)\n",
      "Multinomial Naive Bayes with TF-IDF:\n",
      "----------------------------------------\n",
      "f1: 0.7654\n",
      "accuracy: 0.7666\n",
      "[[ 60  14   6   4]\n",
      " [  8 100   1   1]\n",
      " [  5  12  59   3]\n",
      " [  3  22   2  47]]\n"
     ]
    }
   ],
   "source": [
    "# Answer here\n",
    "# build analyzers (TF-IDF)\n",
    "TF_vectorizer = TfidfVectorizer(max_features=1000)\n",
    "\n",
    "# apply analyzer to training data\n",
    "TF_vectorizer.fit(train_df['text'])\n",
    "\n",
    "#split dataset into train and test\n",
    "X_train = TF_vectorizer.transform(train_df['text'])\n",
    "y_train = train_df['emotion']\n",
    "\n",
    "X_test = TF_vectorizer.transform(test_df['text'])\n",
    "y_test = test_df['emotion']\n",
    "\n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "print('y_test.shape: ', y_test.shape)\n",
    "\n",
    "#build model and train\n",
    "from sklearn.naive_bayes import MultinomialNB # classifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score  # evaluation\n",
    "\n",
    "model_NB = MultinomialNB()\n",
    "\n",
    "model_NB.fit(X_train, y_train)\n",
    "\n",
    "#predict\n",
    "y_pred_tfidf = model_NB.predict(X_test)\n",
    "\n",
    "#caculate accuracy\n",
    "f1 = f1_score(y_test, y_pred_tfidf, average='weighted')\n",
    "accuracy = accuracy_score(y_test, y_pred_tfidf)\n",
    "print('Multinomial Naive Bayes with TF-IDF:')\n",
    "print('-' * 40)\n",
    "print(f'f1: {f1:.4f}')\n",
    "print(f'accuracy: {accuracy:.4f}')\n",
    "\n",
    "## check by confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm_NB = confusion_matrix(y_true=y_test, y_pred=y_pred_tfidf) \n",
    "print(cm_NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 5 (Take home): **  \n",
    "\n",
    "How do the results from the Naive Bayes model and the Decision Tree model compare? How do you interpret these differences? Use the theoretical background covered in class to try and explain these differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navie Bayes has better accuracy than Decision Tree. In my opinion, Decision Tree classifies the sentences step by step according to words, however, it can't tell the importance of words. For example, if a sentence uses double negative way, it may classify wrong. Naive Bayes uses condition probability, so it can calculate the relation between words compare to Decision Tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 6 (Take home): **  \n",
    "\n",
    "Plot the Training and Validation Accuracy and Loss (different plots), just like the images below. How to interpret the graphs you got? How are they related to the concept of overfitting/underfitting covered in class?\n",
    "<table><tr>\n",
    "    <td><img src=\"pics/Accuracy.png\" style=\"width: 300px;\"/> </td>\n",
    "    <td><img src=\"pics/Loss.png\" style=\"width: 300px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as the epoch increaes, the train accuracy also increase, however, the val accuracy doesn't increase. I think it's because overfitting. Train accuracy stays high, but val accuracy can't get higher."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
